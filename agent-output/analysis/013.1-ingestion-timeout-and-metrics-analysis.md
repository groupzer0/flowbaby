# 013.1 Ingestion Timeout and Metrics Analysis

**Value Statement and Business Objective**  
As a Cognee extension user, I want ingestion timeouts and error messages to accurately reflect the true state and performance of the ingestion pipeline, so that I can trust the extension’s feedback, avoid chasing false failures, and continuously improve ingestion latency.

---

## Objective

This analysis defines how to:

1. Increase the ingestion timeout from 30 seconds to 120 seconds.
2. Improve user-facing messages so they distinguish between true failures and timeouts/unknown status.
3. Instrument ingestion to:
   - Log ingestion duration from inside `ingest.py` (Python side).
   - Compare that duration with the extension’s measured duration in `cogneeClient.ts`.
   - Log when `ingest.py` actually exits versus when the timeout triggers in `runPythonScript`.
   - Log metrics for key ingestion steps to identify where time is spent.

This is an incremental improvement and bug fix, scoped as **013.1** building on Plan 013.

---

## Architectural Context

Relevant components:

- `extension/src/cogneeClient.ts`
  - `ingestConversation(...)`: calls `runPythonScript('ingest.py', [...], 30000)` and logs success/failure.
  - `runPythonScript(...)`: spawns Python subprocess, collects stdout/stderr, enforces timeout, logs `Python script timeout` and rejects on timeout.
- `extension/bridge/ingest.py`
  - Performs `cognee.add(...)` + `cognee.cognify(...)` for the workspace-scoped dataset.
  - Returns JSON `{"success": true, "ingested_chars": ..., "timestamp": ...}` on success.
  - Logs structured error details to `stderr` on exceptions.
- `Cognee backend (Python package)`
  - Performs actual ingestion and cognification; internal timing metrics are not currently surfaced to the extension.

Current timeout behavior:

- Ingestion timeout is **30 seconds** (30000 ms) in `ingestConversation` → `runPythonScript`.
- On timeout, the extension logs:
  - `Python script timeout { script: 'ingest.py', timeout: 30000 }`
  - `Ingestion exception { duration: ~30022, error: 'Python script timeout after 30 seconds' }`
- The Python process is **not forcibly killed**; it often completes later and successfully ingests data, leading to false "ingestion failed" perceptions.

---

## Root Cause Clarification

### Confirmed Behavior

- When the user runs **"Capture Cognee Memory"**, the extension sometimes logs a 30s timeout and `Ingestion exception`.
- Subsequent searches using `@cognee-memory` show the ingested content present in the workspace dataset.
- This confirms that **ingestion often completes successfully despite the timeout error**.

### Uncertain Behavior (Must be Measured)

We **do not yet know** whether:

1. Ingestion itself (`cognee.add + cognify`) actually takes longer than 30 seconds, or
2. Ingestion completes within 30 seconds, but:
   - Node’s `spawn`/IO event loop doesn’t receive/process the completion signal (stdout/close) before the timeout, or
   - There is process signaling / buffering latency between Python exit and Node’s `close` event.

Without instrumentation, both explanations remain plausible. Therefore, the plan must:

- Increase the timeout to reduce user-pain immediately.
- Add metrics to disambiguate ingestion latency vs. bridge signaling latency.

---

## Proposed Changes

### 1. Increase Ingestion Timeout to 120 Seconds

**Goal**: Reduce spurious timeouts for legitimate, but slow, ingestion operations.

**Current Code (conceptual)**:

```ts
// In ingestConversation
const result = await this.runPythonScript('ingest.py', [...], 30000);
```

**Change**:

- Increase ingestion timeout from `30000` to `120000` ms.
- Update related comments and README error table (which currently mentions 10s and 30s timeouts).

**Impact**:

- Fewer user-visible timeouts under normal conditions.
- Still protects against indefinitely hung processes (now with 2-minute cap).
- Does **not** fix misleading messaging on its own; must be paired with improved messages and metrics.

---

### 2. Improve User-Facing Messages

**Current behavior**:

- On timeout, `runPythonScript` rejects with `new Error("Python script timeout after 30 seconds")`.
- `ingestConversation` catches and logs:

```ts
this.log('ERROR', 'Ingestion exception', {
  duration,
  error: errorMessage
});
return false;
```

- The extension then likely surfaces a generic failure message to the user.

**Issues**:

- Conflates **"timeout waiting for response"** with **"ingestion failed"**.
- Does not communicate that ingestion may still succeed in the background.

**Proposed behavior**:

- When the error message matches the timeout pattern (e.g., `/Python script timeout after/`):
  - Log a distinct status, e.g., `status: 'timeout_waiting_for_response'`.
  - Surface a user-facing message along the lines of:
    - "Cognee is still working on ingestion in the background. The extension timed out waiting for a response after 120 seconds. Your data may still be ingested; you can check by querying @cognee-memory in a moment."
- For other errors (non-timeout), keep treating as true ingestion failure, but with clearer structured error surfaced (already partially implemented via JSON parsing and structured error logging).

**Implementation hint**:

- In `ingestConversation` catch block:
  - Detect timeout vs non-timeout via error message or a specific error type.
  - Attach a `categorization` field in the log (e.g., `error_type: 'timeout' | 'failure'`).

---

### 3. Instrument Ingestion Duration inside ingest.py

**Goal**: Measure **actual ingestion time** on the Python side, independent of Node’s timeout.

**Where**: In `extension/bridge/ingest.py`:

- Surround the core Cognee calls with timing code:

```python
from time import perf_counter

start_time = perf_counter()

# Cognee operations
await cognee.add(...)
await cognee.cognify(...)

end_time = perf_counter()
ingestion_duration = end_time - start_time
```

- Include this in the JSON success response:

```python
return {
    'success': True,
    'ingested_chars': ingested_chars,
    'timestamp': timestamp,
    'ingestion_duration_sec': ingestion_duration
}
```

- Also log to `stderr` for correlation if needed:

```python
print(f"Ingestion duration: {ingestion_duration:.3f} seconds", file=sys.stderr)
```

**Benefits**:

- Allows direct comparison between **Python-side ingestion duration** and **extension-measured duration** (see next section).
- If ingestion duration is consistently <120s but timeouts still occur, that strongly indicates bridge-level signaling/IO issues.

---

### 4. Compare Python vs Extension Durations

**On the extension side** (`ingestConversation`):

- We already compute:

```ts
const startTime = Date.now();
...
const duration = Date.now() - startTime;
```

- After parsing the result JSON, we will now also have `ingestion_duration_sec`.

**Proposed logging**:

On success:

```ts
this.log('INFO', 'Conversation ingested', {
  chars: result.ingested_chars,
  timestamp: result.timestamp,
  duration_ms: duration,
  ingestion_duration_sec: result.ingestion_duration_sec
});
```

On timeout (exception path), if we can eventually see Python’s success log later, we can correlate by timestamp (for future analysis), but direct correlation may be out-of-scope for this incremental fix. However, even just logging the two durations on success gives us baseline metrics.

**Expected insights**:

- If `duration_ms` ≈ `ingestion_duration_sec * 1000`, then Node is accurately reflecting backend latency.
- If `duration_ms` ≫ `ingestion_duration_sec * 1000` or vice versa, we can identify where overhead is introduced.

---

### 5. Log Process Exit vs Timeout Trigger in runPythonScript

**Goal**: Distinguish between:

1. Timeout firing before process exit, vs
2. Process exit occurring but Node not detecting it in time.

**Current behavior** (`runPythonScript`):

- Logs `Python script timeout` when timer fires.
- Logs `Python script completed` when `python.on('close')` fires.
- On timeout, it calls `reject(...)` and leaves the process running; there is no explicit link between the timeout event and later close event.

**Proposed instrumentation**:

- Add flags and timestamps in `runPythonScript`:

```ts
let timedOut = false;
const requestStart = Date.now();
let timeoutFiredAt: number | null = null;

const timeout = setTimeout(() => {
  timedOut = true;
  timeoutFiredAt = Date.now();
  this.log('ERROR', 'Python script timeout', {
    script: scriptName,
    timeout: timeoutMs,
    elapsed_ms: timeoutFiredAt - requestStart
  });
  reject(new Error(`Python script timeout after ${timeoutMs/1000} seconds`));
}, timeoutMs);

python.on('close', (code) => {
  const closeTime = Date.now();
  this.log('DEBUG', 'Python script completed', {
    script: scriptName,
    exit_code: code,
    close_duration_ms: closeTime - requestStart,
    timed_out,
    timeout_fired_ms: timeoutFiredAt ? timeoutFiredAt - requestStart : null
  });

  if (timedOut) {
    // Process completed after promise was already rejected; just return.
    return;
  }

  clearTimeout(timeout);
  // Existing success/error handling...
});
```

**What this tells us**:

- If `timed_out === true` and `close_duration_ms > timeoutMs`, we know the Python process genuinely ran longer than the timeout.
- If `timed_out === true` but `close_duration_ms <= timeoutMs`, then we have evidence of race conditions or delayed event handling.

---

### 6. Step-Level Ingestion Metrics inside ingest.py

**Goal**: Understand where time is spent inside the ingestion pipeline.

**Key steps** (from `ingest.py`):

1. Load `.env` and environment (`LLM_API_KEY`, etc.).
2. Import `cognee` and configure system/data directories.
3. Configure LLM provider and API key.
4. Generate dataset name and resolve ontology.
5. `await cognee.add(...)` (data ingestion).
6. `await cognee.cognify(datasets=[dataset_name])` (cognification).

**Proposed instrumentation**:

Use `perf_counter()` and log durations for each step, aggregated into a single metrics object, printed to `stderr` and optionally included in the JSON (if size stays reasonable):

```python
from time import perf_counter

metrics = {}
overall_start = perf_counter()

step_start = perf_counter()
# 1. Load env / API key
...
metrics['load_env_sec'] = perf_counter() - step_start

step_start = perf_counter()
# 2. Import cognee & configure dirs
...
metrics['init_cognee_sec'] = perf_counter() - step_start

step_start = perf_counter()
# 3. Configure LLM provider/API key
...
metrics['config_llm_sec'] = perf_counter() - step_start

step_start = perf_counter()
# 4. Dataset name + ontology resolution
...
metrics['dataset_ontology_sec'] = perf_counter() - step_start

step_start = perf_counter()
# 5. cognee.add
await cognee.add(...)
metrics['add_sec'] = perf_counter() - step_start

step_start = perf_counter()
# 6. cognee.cognify
await cognee.cognify(datasets=[dataset_name])
metrics['cognify_sec'] = perf_counter() - step_start

metrics['total_ingest_sec'] = perf_counter() - overall_start

print(f"Ingestion metrics: {json.dumps(metrics)}", file=sys.stderr)
```

Include a summary in the JSON response:

```python
return {
    'success': True,
    'ingested_chars': ingested_chars,
    'timestamp': timestamp,
    'ingestion_duration_sec': metrics['total_ingest_sec'],
    'ingestion_metrics': metrics
}
```

On the extension side, log these metrics (possibly truncated) for deeper analysis when debugging.

**Benefits**:

- Identifies whether most time is spent in `add` vs `cognify` vs initialization.
- Helps inform future optimization plans (e.g., caching, background initialization, splitting commands).

---

## Strategic Considerations

- **Incremental scope (013.1)**: This is a narrow, high-value improvement focused on reliability and observability. It does not change functional behavior of Cognee’s ingestion logic, only the extension’s timeout behavior and logging.
- **User trust**: Clarifying timeout vs failure significantly improves user trust in the system.
- **Future work enablement**: Step-level metrics and Python vs Node duration comparisons provide the necessary evidence base for a future performance optimization plan.

---

## Recommendations

1. **Increase ingestion timeout from 30s to 120s** in `ingestConversation` and update documentation.
2. **Differentiate timeout vs failure** in `ingestConversation` and user-facing messages.
3. **Add ingestion duration and step-level metrics** inside `ingest.py`, including `ingestion_duration_sec` and `ingestion_metrics` in the JSON response.
4. **Log process exit vs timeout** in `runPythonScript` with `timedOut`, `timeout_fired_ms`, and `close_duration_ms`.
5. Use these metrics to decide in a later plan whether to:
   - Further adjust timeouts.
   - Split ingestion into smaller, user-visible phases.
   - Optimize specific slow steps (`add`, `cognify`, ontology handling, etc.).

---

## Scope Considerations

- This analysis is intentionally scoped as **013.1 (incremental fix)** building on Plan 013.
- It does **not** propose large-scale architectural changes or new user-facing features.
- It focuses on observability and error semantics, which are prerequisites for deeper performance work.

---

## Open Questions

1. Do we want to make the ingestion timeout **user-configurable** (e.g., via extension settings), or is 120 seconds acceptable as a fixed value for now?
2. How much of the step-level metrics should be surfaced to users (e.g., in a debug view) versus logged only in the output channel?
3. Should timeouts trigger a **background status check** (e.g., a quick retrieve) before telling the user anything definitive about failure?

---

## References

- `extension/src/cogneeClient.ts` – `ingestConversation`, `runPythonScript`
- `extension/bridge/ingest.py` – Cognee ingestion bridge script
- `extension/CHANGELOG.md` – Context on previous ingestion timeout fixes and workspace-local storage
