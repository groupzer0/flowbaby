# Value Statement and Business Objective

As a VS Code extension user, I want Cognee Memory v0.2.0 to store and retrieve my relevant information reliably in my workspace, so that the @cognee-memory participant can answer questions with accurate, fast context.

## Objective

Investigate why v0.2.0 appears not to store data and returns unhelpful retrievals, and why retrieval latency is high (>10s), then provide clear verification steps and corrective actions to ensure reliable storage and relevant, low-latency retrievals.

## Methodology

- Reviewed your Output Channel logs (workspace: `/home/luke/Documents/Github-projects/writing`) for initialization, ingestion, and retrieval telemetry
- Cross-referenced extension behaviors with v0.2.0 design (manual capture vs. optional auto-ingest)
- Validated retrieval pipeline in code (`CogneeClient.retrieve`) to confirm how context arrays are populated and logged
- Compared expected vs. observed outcomes and mapped to likely root causes
- Prepared a minimal end-to-end verification protocol (clear ‚Üí capture ‚Üí retrieve) tailored to your workspace

## Findings

### 1. Your logs show successful initialization and workspace isolation, but no confirmed ingestions

- Logs:
- "CogneeClient initialized" and "Initialized workspace ‚Ä¶" indicate environment is healthy (Python path autodetected, ontology loaded, migration complete).
- "Context retrieved { result_count: 1 }" shows the retriever found 1 item, but that item content appears to be a generic response (e.g., ‚ÄúI don't have any information about commit 8aa82b1‚Ä¶‚Äù), not your domain content.
- Interpretation: The dataset likely contains earlier or generic content, but not your new target content (e.g., commit 8aa82b1), because it wasn't captured.

### 2. Retrieval latency (~10‚Äì12s) suggests cold-start or heavyweight import costs

- The extension uses a Python bridge importing Cognee + RDF tooling; the first retrievals can be slow due to interpreter startup, imports, and any index hydration.
- The client logs a WARN when >1000ms; your observed 10‚Äì12s is high. This suggests latency is dominated by Python interpreter startup and imports (cold-start), rather than pure query execution. Subsequent calls should improve, but may still be >1s on some systems.

### 3. What ‚ÄúRetrieved 1 memory‚Äù means

- In code, `retrieve.py` returns an array of results; the extension maps them to `r.text` and shows them as the memory preview.
- If that preview shows a generic explanation (‚ÄúI don't have info about commit‚Ä¶‚Äù), that text is literally the stored memory for that result, not the generated reply. It likely comes from a previous capture (intentional or accidental) in this workspace.

### 4. Verified Cognee 0.4.0 Python API signatures and expected parameters

- add: `async def add(data, dataset_name: str = "main_dataset", ...)`.
  - Accepts text, file paths, URLs, or lists. Use `dataset_name` (string) to route content into a dataset. It does not accept `datasets=[...]` during add.
  - Source: `cognee/api/v1/add/add.py` (v0.4.0).
- cognify: `async def cognify(datasets: Union[str, list[str], list[UUID]] = None, ...)`.
  - Scopes processing via `datasets` (one name, a list, or UUIDs). No `ontology_file_path` parameter in the function signature; ontology is discovered from environment via `ontology_env_config` (`ontology_file_path`, `ontology_resolver`, `matching_strategy`).
  - Source: `cognee/api/v1/cognify/cognify.py` + `cognee/modules/ontology/ontology_env_config.py` (v0.4.0).
- search: `async def search(query_text, ..., datasets: Optional[Union[list[str], str]] = None, ...)`.
  - Scopes queries via `datasets` (string or list). LLM is only required for certain query types; pure chunk/graph traversal can return without LLM calls.
  - Source: `cognee/api/v1/search/search.py` (v0.4.0).

### 5. Likely ingestion failure causes in our current bridge (corroborated against v0.4.0)

- Env var mismatch: Cognee 0.4.0 documentation and code expect `LLM_API_KEY` (and optionally `EMBEDDING_API_KEY`). Our bridge only checks `OPENAI_API_KEY` before proceeding. If the user followed official docs and set `LLM_API_KEY` but not `OPENAI_API_KEY`, the bridge returns `OPENAI_API_KEY not found‚Ä¶` and aborts ingestion.
- Wrong kwarg on add: Our bridge first tries `await cognee.add(data=[conversation], datasets=[dataset_name])` which is not a valid parameter for `add()` in 0.4.0 (expects `dataset_name`, not `datasets`). We do catch `TypeError` and retry with `dataset_name=...` then `dataset=...`, so it should eventually succeed, but the initial error is noisy and slows ingestion.
- Cognify ontology handling: We pass `ontology_file_path` into `cognify()`. In v0.4.0 that kwarg is not in the function signature. We catch the `TypeError` and retry without it, but the correct approach is to set `ontology_file_path` in the environment (e.g., `.env`) so `cognify()` reads it via `OntologyEnvConfig`.
- Net effect: Depending on which condition hits first, this may result in users seeing a generic ‚ÄúIngestion failed‚Äù even though our fallbacks eventually correct parameter shapes. If `LLM_API_KEY` is missing (but `OPENAI_API_KEY` is present or absent), cognify will still fail at entity extraction and graph generation.

### 6. Confirmed environment requirements for 0.4.0

- LLM key: Prefer `LLM_API_KEY` (docs default). If only `OPENAI_API_KEY` is present, we can programmatically set the LLM key via `cognee.config.set_llm_api_key(...)` (our bridge already does this) ‚Äî but we should also read `LLM_API_KEY` directly to align with docs.
- Provider/model defaults: If you don‚Äôt set provider explicitly, Cognee defaults to OpenAI for both LLM and embeddings (docs). Our bridge sets provider to `openai` explicitly; that is acceptable.
- Ontology: To use an ontology file, set `ontology_file_path` in the environment (either key `ontology_file_path` in `.env` or `ONTOLOGY_FILE_PATH` env var). v0.4.0 reads this value via `OntologyEnvConfig` and configures the resolver automatically.

### 7. Observed ingestion errors and timeouts from Output ‚Üí Cognee Memory

- Repeated 30s Python script timeouts during `ingest.py`.
- Structured error shows `File not found` under venv site-packages:
  - `.../.venv/lib/python3.12/site-packages/cognee/.data_storage/text_649c....txt`
  - Storage path exists, but available files differ (e.g., `text_09c6....txt`).
- Retrieval returns `result_count: 1` with long latency (7‚Äì12s), consistent with cold start and non-local storage penalties.

### 8. Root cause: Cognee system/data directories are not scoped to the workspace

- The bridge `init.py` configures API key and creates a local `.cognee` marker directory, but it does not call:
  - `cognee.config.system_root_directory(<workspace>/.cognee_system)`
  - `cognee.config.data_root_directory(<workspace>/.cognee_data)`
- Without these calls, Cognee uses defaults in site-packages (global path). That leads to:
  - Cross-context file lookup mismatches (hash `text_649c...` requested vs only `text_09c6...` present).
  - Slower IO and occasional permission/path issues.
  - Ingestion failures and repeated timeouts.

## Recommendations

B) Triage high retrieval latency

1. Warm-up step: Perform one retrieval after VS Code starts to pay the import/startup cost; subsequent retrievals are usually faster.
2. Reduce load: Keep `maxContextResults = 1` (default is 3 in some configs) and ensure `maxContextTokens` isn't excessively large.
3. Environment sanity:

- Ensure venv is activated/healthy (your logs show autodetect OK)
- Confirm Cognee is installed and up to date inside the venv: `python -c "import cognee; print(cognee.__version__)"`
- SSD-backed workspace and sufficient RAM help reduce interpreter startup time.


C) Sanity checks on data presence

- Filesystem:
  - Verify workspace data folder exists: `/home/luke/Documents/Github-projects/writing/.cognee`
  - Observe size changes after captures: `du -sh /home/luke/Documents/Github-projects/writing/.cognee`

- Log cues during capture:
  - Success: `Conversation ingested { chars: N, timestamp: ... }`
  - Failure: `Ingestion failed` or `Ingestion exception` with details (now visible in Output channel)

D) Scope clarification (why commits weren‚Äôt found)

- The extension doesn‚Äôt crawl your Git history or repo automatically. To answer questions like ‚Äútell me about commit 8aa82b1,‚Äù you must capture relevant snippets (commit messages/diffs/summaries) via Ctrl+Alt+C. Retrieval uses only what‚Äôs been captured into your workspace‚Äôs `.cognee` dataset.

E) Implementation-ready fixes for the ingestion bridge (explicit errors, no silent fallbacks)

- Require `LLM_API_KEY` (no fallback): The bridge should validate `LLM_API_KEY` is set and fail fast with a clear error if missing. Do not fall back to `OPENAI_API_KEY`. If you prefer using `OPENAI_API_KEY`, mirror it in `.env` as `LLM_API_KEY` explicitly.
- add(): Call with `dataset_name=...` only. Do not attempt `datasets=[...]` or alternate kwarg retries; let signature mismatches surface as errors.
- cognify(): Call with `datasets=[dataset_name]` only. Do not pass `ontology_file_path` as a kwarg; set `ontology_file_path` in `.env` instead. Do not catch-and-retry TypeErrors‚Äîsurface them.
- Logging: When ingestion fails, include the exception class, message, and the exact parameters used. Do not mask failures through retries.

F) Workspace-local storage configuration (critical)

- In `extension/bridge/init.py`, after setting the LLM key/provider, set Cognee‚Äôs directories to the workspace:
  - `cognee.config.system_root_directory(str(Path(workspace_path) / '.cognee_system'))`
  - `cognee.config.data_root_directory(str(Path(workspace_path) / '.cognee_data'))`
- Keep `.cognee/` for local markers as-is. This ensures all databases and data storage are per-workspace, eliminates site-packages pathing, and prevents cross-context file mismatches.

G) Retest with the Barcelona text

- Clear ‚Üí Capture ‚Üí Retrieve using the same sentence.
- Expected: No timeouts, no `File not found` in site-packages; `üìö Retrieved 1 memory` includes the Barcelona snippet; latency improves after first run.

H) Minimal Python E2E to validate your environment (optional, environment-level validation)

- Pre-reqs in `.env` at your workspace root:
  - `LLM_API_KEY="sk-..."`
  - optionally `ontology_file_path="/absolute/path/to/ontology.ttl"`
- Script skeleton:
  - `import asyncio, cognee`
  - `async def main():`
    - `await cognee.add("Hello from VS Code extension test", dataset_name="test_ws")`
    - `await cognee.cognify(datasets=["test_ws"])`
    - `res = await cognee.search("What did I say?", datasets=["test_ws"])`
    - `print(res[:1])`
  - `asyncio.run(main())`

Expected: add + cognify complete without TypeError; search returns a context snippet containing your test string. This serves as an environment-level sanity check beyond the VS Code extension itself.

## Scope Considerations

The following items are related to the overall experience but are **outside the minimal scope required to fix the v0.2.0 ingestion and workspace-storage issue**. They are worth considering as follow-up improvements:

- Automatic conversation capture defaults (e.g., enabling `cogneeMemory.autoIngestConversations` by default in some workspaces).
- Hardware and environment tuning (SSD-backed workspace, available RAM) for additional latency improvements beyond the core cold-start and storage-path fixes.
- The standalone Python E2E script in section H, which validates the Cognee library and environment directly, independent of the VS Code extension.

## Verification Checklist (expected signals)

- After Clear:
  - Output Channel: `clearMemory` success and `.cognee` resized/emptied
- After Capture:
  - Output Channel: `Conversation ingested` with `chars`
  - `.cognee` directory grows in size
- After Retrieve:
  - Output Channel: `Context retrieved { result_count: >=1 }`
  - Chat preview shows your captured text snippets
  - Latency for second/third retrieval improves vs. first run

## Open Questions

1. **Scope consideration**: Do you want automatic conversation capture on by default in this workspace? If yes, we can enable `cogneeMemory.autoIngestConversations` and closely monitor.
2. Are you seeing ingestion failures in the Output Channel when using Ctrl+Alt+C? If so, please share the exact error lines.
3. Does retrieval latency remain >5s after 2‚Äì3 warm-up queries? If it does, we should profile the Python side (imports, model loading) and consider caching strategies.

## References

- v0.2.0 behaviors and settings (package.json contributes ‚Üí configuration)
- CogneeClient code paths: `ingest()` and `retrieve()` (maps Python results to `r.text`)
- Known limitation: Auto-ingest disabled by default due to Cognee 0.4.0 file hashing bug
- Developer docs: `extension/SETUP.md` (debugging, Output Channel signals)
- Distribution/Usage: `extension/README.md` (keyboard shortcut capture, participant usage)

- Cognee 0.4.0 PyPI metadata (project links, default env var names): <https://pypi.org/pypi/cognee/0.4.0/json>
- Cognee repository (v0.4.0):
  - API entry points: `cognee/__init__.py` ‚Üí imports for `add`, `cognify`, `search`, `datasets`, `config`
  - add signature and docs: <https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/main/cognee/api/v1/add/add.py>
  - cognify signature and docs: <https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/main/cognee/api/v1/cognify/cognify.py>
  - search signature and docs: <https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/main/cognee/api/v1/search/search.py>
  - ontology env config: <https://raw.githubusercontent.com/topoteretes/cognee/refs/heads/main/cognee/modules/ontology/ontology_env_config.py>
- Cognee Docs (selected):
  - Quickstart: <https://docs.cognee.ai/getting-started/quickstart>
  - Installation and env: <https://docs.cognee.ai/getting-started/installation>
  - LLM Providers and env var notes: <https://docs.cognee.ai/setup-configuration/llm-providers>
  - Datasets concept: <https://docs.cognee.ai/core-concepts/further-concepts/datasets>
